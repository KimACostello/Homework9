---
title: "Homework 9: Modeling Practice"
author: "Kim Costello"
format: html
editor: visual
---

### Seoul Bike Sharing Demand Dataset

```{r}
library(readr)
library(broom)
library(lubridate)
library(janitor)
library(dplyr)
library(ggplot2)
library(GGally)
library(tidymodels)
```

The following data set is provided by the UCI Machine Learning Repository. The data set contains count of public bicycles rented per hours in the Seoul Bike Sharing System, with corresponding weather data and holiday information.

The following variables are provided:

-   Date - day/month/year

-   Rented Bike count - Count of bikes rented at each hour

-   Hour - Hour of the day

-   Temperature-Temperature in Celsius

-   Humidity - %

-   Windspeed - m/s

-   Visibility - 10m

-   Dew point temperature - Celsius

-   Solar radiation - MJ/m2

-   Rainfall - mm

-   Snowfall - cm

-   Seasons - Winter, Spring, Summer, Autumn

-   Holiday - Holiday/No holiday

-   Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)

```{r}
# Read in data
bike_data <- read_csv("SeoulBikeData.csv", locale = locale(encoding = "latin1"))
```

#### EDA

Before we summarize the data, let's determine if the data set contains any missing values.

```{r}
# check for missing values
colSums(is.na(bike_data))
```

The data provided does not contain any missing values.

The following contains the structure of the data set, so we can see the different variables and how they are being stored.

```{r}
str(bike_data)
```

Below is a summary of basic stats that can be used to make sure the data makes sense.

```{r}
summary(bike_data)
```

```{r}
unique(bike_data$Seasons)
unique(bike_data$Holiday)
unique(bike_data$`Functioning Day`)
```

Unique values for Seasons are Winter, Spring, Summer, and Autumn. The holiday variable value for an observation is either No Holiday or Holiday. The Functioning Day is either Yes or No (Yes, it is during functional hours or No, it is not).

Let's manipulate the data by:

-   converting the date column to an actual date.

-   turning the character variables into factors (for the different levels/categories).

-   renaming all the variables to snake case.

```{r}
bike_data <- bike_data |>
  mutate(
     Date = dmy(Date),
     Seasons = as.factor(Seasons),
     Holiday = as.factor(Holiday),
     `Functioning Day` = as.factor(`Functioning Day`)
  ) |>
  clean_names("snake")
  
```

Let's see how many observations occurred in each season during functional or non-functional hours. And let's see how many observations occurred during a holiday vs no holiday during functional and non-functional hours.

```{r}
# Two-way contingency tables
table(bike_data$functioning_day, bike_data$seasons)
table(bike_data$functioning_day, bike_data$holiday)
```

There are no observations in Summer and Winter that occurred during non-functional hours. Most observations are not on a holiday and during functional hours.

Let's look at the Mean and Median for all numeric variables during functional and non-functional hours.

```{r}
bike_data |>
  group_by(functioning_day) |>
  summarize(across(where(is.numeric), 
                   list("mean" = mean, "median" = median), 
                   .names = "{.fn}_{.col}"))
```

The mean and median rented bike counts are zero during non-functional hours.

To confirm there are no bikes being rented during non-functional hours, let's look at the min and max rental bike counts.

```{r}
bike_data |>
  group_by(functioning_day) |>
  summarize(min_bike_count = min(rented_bike_count), max_bike_count = max(rented_bike_count))
  
```

No bikes were rented during non-functional hours.

Since no bikes are being rented during non-functional hours, we can remove the observations that occurred during non-functional hours.

```{r}
fun_bike_data <- bike_data |>
  filter(functioning_day == "Yes")
```

To simplify analysis, let's summarize across the hours so each day has one observation with it. This can be done by grouping by character variables, and summarizing (either sum or mean) across numeric variables.

```{r}
bike_summary_data <- fun_bike_data |>
  group_by(date, seasons, holiday) |>
  summarize(
    sum_bike_count = sum(rented_bike_count),
    sum_rainfall = sum(rainfall_mm),
    sum_snowfall = sum(snowfall_cm),
    across(c(temperature_c, 
             humidity_percent,
             wind_speed_m_s,
             visibility_10m,
             dew_point_temperature_c,
             solar_radiation_mj_m2), 
           ~ mean(.x),
           .names = "mean_{.col}")
    
  )

head(bike_summary_data)

    
```

Now that non-functional hours have been removed from the data, let's see the number of observations that occurred on a holiday or no holiday by the season.

```{r}
table(bike_summary_data$seasons, bike_summary_data$holiday)
```

Let's get the mean and median of numeric variables without the non-functional hours observations, by seasons.

```{r}
bike_summary_data |>
  group_by(seasons) |>
  summarize(across(where(is.numeric), 
                   list("mean" = mean, "median" = median), 
                   .names = "{.fn}_{.col}"))
```

Not surprisingly, the most bike rentals occurred during the Summer, then Autumn.

Let's see the min and max daily bike rentals by season.

```{r}
bike_summary_data |>
  group_by(seasons) |>
  summarize(min_bike_count = min(sum_bike_count), max_bike_count = max(sum_bike_count))
```

Let's generate a correlation matrix plot, so we can see the relationships between the numeric variables.

```{r}
df_corr <- bike_summary_data |>
  select(where(is.numeric)) |>
  rename("Bike_count" = sum_bike_count,
         "Rainfall" = sum_rainfall,
         "Snowfall" = sum_snowfall,
         "Temperature" = mean_temperature_c,
         "Humidity" = mean_humidity_percent,
         "Windspeed" = mean_wind_speed_m_s,
         "Visibility" = mean_visibility_10m,
         "Dew_point" = mean_dew_point_temperature_c,
         "Solar_radiation" = mean_solar_radiation_mj_m2)

ggcorr(df_corr, 
       nbreaks = 6,
       label = TRUE,
       palette = "BuPu",
       size = 2,          # adjusting variable names sizes
       hjust = 0.65) +    # adjusting position of variable names) 
  labs(title = "Correlation Matrix of all Numeric Variables") +
  ggeasy::easy_center_title()
```

When looking at daily bike rentals, temperature has the greatest positive correlation (the higher the temp, the higher the \# of bike rentals). Humidity does not appear to have any relationship with bike rental counts.

```{r}
df_corr |>
  group_by(seasons) |>
  summarize(correlation = cor(Bike_count, Temperature, use = "pairwise.complete.obs"))
```

When looking at the relationship between bike counts and temperature across seasons, it's surprising to see that summer has a slight negative correlation. Spring has a greater positive correlation between temperature and bike counts.

Let's see some plots to explore the data.

Scatterplot:

```{r}
g <- ggplot(data = bike_summary_data)

g + geom_point(aes(x = sum_bike_count, 
                   y = sum_rainfall, 
                   color = seasons)) +
  labs(x = "Rental Bike Count", 
       y = "Rainfall (mm)",
       title = "Rental Bike Count by Total Daily Rainfall",
       color = "Seasons") +
  scale_color_manual(values = c("#FFFF00",
                                "#FF00FF",
                                "#7FFF00",
                                "#00FFFF")) +
  theme_dark() +
  ggeasy::easy_center_title() 
```

In the plot above, you can see rental bike counts vs rainfall. The points are also color coded by season. Bike rentals are greater in the summer, but there are fewer observations during heavy rainfall.

Boxplot:

```{r}
g + geom_boxplot(aes(x = seasons, y = sum_bike_count, fill = holiday)) +
  labs(x = "Seasons", 
       y = "Daily Rental Bike Count",
       title = "Daily Rental Bike Count by Seasons",
       fill = "Holiday") +
  ggeasy::easy_center_title() +
  scale_fill_manual(values = c("lightpink",
                               "lightblue"))
```

In the plot above, you can see that bike rental counts are greater in Summer and Autumn. Spring has a larger range of bike counts on a given day (greater distance between the min and max).

Scatterplot with a regression line:

```{r}
ggplot(bike_summary_data, aes(x = date, y = sum_bike_count)) +
  geom_point() +
  labs(x = "Date", 
       y = "Daily Rental Bike Count",
       title = "Daily Rental Bike Count by Date") +
  ggeasy::easy_center_title() +
  geom_smooth()
```

In the plot above, you can see the trend of daily bike rentals by date, throughout 2018. Between June and July appear to have the most daily bike rentals, and January has the lowest.

#### Splitting the Data

Split the data into training and test set.

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(123)
# Put 3/4 of the data into the training set 
data_split <- initial_split(bike_summary_data, prop = 3/4, strata = seasons)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)

# Create 10-fold cross validation split on the training data
bike_folds <- vfold_cv(train_data, 10)
```

#### Fitting MLR Models

Let's create some recipes.

```{r}
recipe_one <- recipe(sum_bike_count ~ ., data = train_data) |> 
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow") |>
  step_mutate(day_of_week = 
                as.factor(ifelse(date_dow %in% c("Sat", "Sun"), "weekend", "weekday"))) |>
  step_rm(date_dow) |>     #removes intermediate variable
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_of_week)

```

```{r}
# Same as recipe one except including interactions.
recipe_two <- recipe(sum_bike_count ~ ., data = train_data) |> 
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow") |>
  step_mutate(day_of_week = 
                as.factor(ifelse(date_dow %in% c("Sat", "Sun"), "weekend", "weekday"))) |>
  step_rm(date_dow) |>     #removes intermediate variable
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_of_week) |>
  step_interact(terms = ~starts_with("holiday")*starts_with("seasons") +
                  ~mean_temperature_c*starts_with("seasons") +
                  ~mean_temperature_c*sum_rainfall)
  

```

```{r}
# Same as recipe two except adding quadratic terms to numeric predictors.
recipe_three <- recipe(sum_bike_count ~ ., data = train_data) |> 
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow") |>
  step_mutate(day_of_week = 
                as.factor(ifelse(date_dow %in% c("Sat", "Sun"), "weekend", "weekday"))) |>
  step_rm(date_dow) |>     #removes intermediate variable
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_of_week) |>
  step_interact(terms = ~starts_with("holiday")*starts_with("seasons") +
                  ~mean_temperature_c*starts_with("seasons") +
                  ~mean_temperature_c*sum_rainfall) |>
  step_poly(starts_with("mean"), sum_rainfall, sum_snowfall)
```

Now, let's set up our linear model fit to use the lm engine.

```{r}
MLR_spec <- linear_reg() |>
  set_engine("lm")
```

Combining recipes and model into workflows (for each recipe).

```{r}
workflow_one <- workflow() |>
  add_recipe(recipe_one) |>
  add_model(MLR_spec)

workflow_two <- workflow() |>
  add_recipe(recipe_two) |>
  add_model(MLR_spec)

workflow_three <- workflow() |>
  add_recipe(recipe_three) |>
  add_model(MLR_spec)

```

Fit the models using the 10 fold CV.

```{r}
bike_CV_fit_one <- workflow_one |>
  fit_resamples(bike_folds)
bike_CV_fit_one

bike_CV_fit_one |>
  collect_metrics()
```

```{r}
bike_CV_fit_two <- workflow_two |>
  fit_resamples(bike_folds)
bike_CV_fit_two

bike_CV_fit_two |>
  collect_metrics()
```

```{r}
bike_CV_fit_three <- workflow_three |>
  fit_resamples(bike_folds)
bike_CV_fit_three

bike_CV_fit_three |>
  collect_metrics()
```

Model 3 is the best fit (lowest RMSE and highest RSQ).

Using the "best" model, fit the model to the entire training data set.

```{r}
best_fit <- 
  workflow_three |>
  last_fit(data_split)

best_fit |>
  collect_metrics()
```

Obtain the final model coefficient table.

```{r}
best_fit |>
  extract_fit_parsnip() |>
  tidy()
```

------------------------------------------------------------------------

### Homework 9

#### LASSO model

```{r}
# set up model
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

#create workflow
LASSO_wkf <- workflow() |>
  add_recipe(recipe_one) |>
  add_model(LASSO_spec)
LASSO_wkf
```

```{r}
# Fit the model
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = bike_folds,
            grid = grid_regular(penalty(), levels = 200)) 

LASSO_grid[1, ".metrics"][[1]]

```

```{r}
# Pull out the best model
best_LASSO <- LASSO_grid |>
  select_best(metric = "rmse")
best_LASSO
```

```{r}
# Fit best LASSO model on the entire training set.
LASSO_final_fit <- LASSO_wkf |>
  finalize_workflow(best_LASSO) |>
  last_fit(data_split, metrics = metric_set(rmse, mae))

LASSO_final_fit |>
  collect_metrics()
```

#### Regression Tree Model

```{r}
# Define model and engine
tree_mod <- decision_tree(tree_depth = tune(),
                            min_n = 20,
                            cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")

# Create Workflow
tree_wkf <- workflow() |>
  add_recipe(recipe_one) |>
  add_model(tree_mod)

# Fit the workflow to the CV folds
tree_fit <- tree_wkf |> 
  tune_grid(resamples = bike_folds)

tree_fit |> 
  collect_metrics()|>
  filter(.metric == "rmse") |>
  arrange(mean)
```

```{r}
# Get the best model's tuning parameter values
best_tree <- select_best(tree_fit, metric = "rmse")
best_tree
```

```{r}
# Finalize workflow and fit to entire training set
tree_final_wkf <- tree_wkf |>
  finalize_workflow(best_tree)

tree_final_fit <- tree_final_wkf |>
  last_fit(data_split, metrics = metric_set(rmse, mae))
tree_final_fit

tree_final_fit |>
  collect_metrics()
```

#### Bagged Tree model

```{r}
# Set up model and engine
library(baguette)
bag_mod <- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")

# Create workflow
bag_wkf <- workflow() |>
 add_recipe(recipe_one) |>
 add_model(bag_mod)

# Fit to CV folds
bag_fit <- bag_wkf |>
  tune_grid(resamples = bike_folds,
            grid = grid_regular(cost_complexity(),  #tuning parameter of interest
                                levels = 15),    #it will choose 15 values
            metrics = metric_set(rmse, mae))

# Check metrics
bag_fit |>
  collect_metrics() |>
  arrange(mean)
```

```{r}
# Get best tuning parameter
bag_best <- select_best(bag_fit, metric = "rmse")
bag_best
```

```{r}
# Refit on the entire training set using the tuning parameter
bag_final_wkf <- bag_wkf |>
  finalize_workflow(bag_best)

bag_final_fit <- bag_final_wkf |>
  last_fit(data_split, metrics = metric_set(rmse, mae))
#last fit fits to training data and tests on test data

bag_final_fit |>
  collect_metrics()
```

#### Random Forest Model

```{r}
#Extract function
get_rf_imp <- function(x) {
    x %>% 
        extract_fit_parsnip() %>% 
        vip::vi()
}

# Set up model and engine
rf_mod <- rand_forest(mtry = tune()) |>
 set_engine("ranger", importance = "impurity") |>
 set_mode("regression")

# This is to get the variable importance 
ctrl_imp <- control_grid(extract = get_rf_imp)

#create workflows
 rf_wkf <- workflow() |>
   add_recipe(recipe_one) |>
   add_model(rf_mod)
 
 # Fit CV folds
 rf_fit <- rf_wkf |>
  tune_grid(resamples = bike_folds,
            grid = 7,  #7 total values
            metrics = metric_set(rmse, mae),
            control = ctrl_imp)    # variable importance
 
 # Check metrics across folds
 rf_fit |>
  collect_metrics() |>
  arrange(mean)
```

```{r}
# Get best tuning parameter
rf_best <- select_best(rf_fit, metric = "rmse")
rf_best
```

```{r}
# Refit on entire training set using the tuning parameter.
rf_final_wkf <- rf_wkf |>
 finalize_workflow(rf_best)
rf_final_fit <- rf_final_wkf |>
 last_fit(data_split, metrics = metric_set(rmse, mae))

rf_final_fit |>
  collect_metrics()
```

#### MLR Model

```{r}
# Fit MLR best model on the entire training data set, and get RMSE and MAE metrics
MLR_final_fit <- workflow_three |>
  last_fit(data_split, metrics = metric_set(rmse, mae))
```

#### Compare models.

```{r}
rbind(
  MLR_final_fit |>
    collect_metrics() |> 
    mutate(Model = "MLR", .before = ".metric"),
  LASSO_final_fit |>
    collect_metrics() |> 
    mutate(Model = "LASSO", .before = ".metric"), 
  tree_final_fit |>
    collect_metrics() |> 
    mutate(Model = "TREE", .before = ".metric"), 
  bag_final_fit |>
    collect_metrics() |> 
    mutate(Model = "BAG", .before = ".metric"),
  rf_final_fit |>
    collect_metrics() |> 
    mutate(Model = "RF", .before = ".metric")
) |>
  arrange(.metric, .estimate)
```

The Random Forest Model is the best model. 


#### Extracting the final model fits for each type

MLR Model, with final coefficient table: 
```{r}
MLR_final_model <- extract_workflow(MLR_final_fit) 
MLR_final_model

tidy(MLR_final_model)

```


LASSO Model, with final coefficient table:
```{r}
LASSO_final_model <- extract_fit_parsnip(LASSO_final_fit) 
LASSO_final_model

tidy(LASSO_final_model)
```


Regression Tree Model: 
```{r}
tree_final_model <- extract_workflow(tree_final_fit) 
tree_final_model
```

Plot of final fit for the regression tree model: 
```{r}
tree_final_model |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = FALSE)
```

Bagged tree model, with a variable importance plot: 
```{r}
bag_final_model <- extract_fit_engine(bag_final_fit)

bag_final_model$imp |>
  mutate(term = factor(term, levels = term)) |>
  ggplot(aes(x = term, y = value)) +
  geom_bar(stat ="identity") +
  coord_flip()
```
Temperature was the most important to # of bike rentals. 

Random Forest Model variable importance plot: 
```{r}
rf_fit %>%
    select(id, .extracts) %>%
    unnest(.extracts) %>%
    unnest(.extracts) %>%
    group_by(Variable) %>%
    summarise(Mean = mean(Importance),
              Variance = sd(Importance)) %>%
    slice_max(Mean, n = 15) %>%
    ggplot(aes(Mean, reorder(Variable, Mean))) +
    geom_crossbar(aes(xmin = Mean - Variance, xmax = Mean + Variance)) +
    labs(x = "Variable importance", y = NULL)
```


Fit the best model to the entire data set!
```{r}

```

